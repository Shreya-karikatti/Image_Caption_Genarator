# -*- coding: utf-8 -*-
"""Vgg16imagecap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19aw9UYCV_3pHPjyTpyf200gted_-kwdx
"""

from google.colab import auth
auth.authenticate_user()

from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/Othercomputers/My Laptop/Desktop/project"

"""#Import Modules

"""

import os   # handling the files
import pickle # storing numpy features
import numpy as np
from tqdm.notebook import tqdm # how much data is process till now
from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.
from tensorflow.keras.preprocessing.image import load_img , img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add

"""os -  used to handle files using system commands.

pickle -  used to store numpy features extracted

numpy -  used to perform a wide variety of mathematical operations on arrays

tqdm -  progress bar decorator for iterators. Includes a default range iterator printing to stderr.

VGG16, preprocess_input -  imported modules for feature extraction from the image data

load_img, img_to_array -  used for loading the image and converting the image to a numpy array

Tokenizer -  used for loading the text as convert them into a token

pad_sequences -  used for equal distribution of words in sentences filling the remaining spaces with zeros

plot_model -  used to visualize the architecture of the model through different images
"""

BASE_DIR ="/content/drive/Othercomputers/My Laptop/Desktop/ICG"
WORKING_DIR = "/content/drive/Othercomputers/My Laptop/Desktop/ICG"

# Load vgg16 Model
model = VGG16()

# restructure model
model = Model(inputs = model.inputs , outputs = model.layers[-2].output)

# Summerize
print(model.summary())

"""Fully connected layer of the VGG16 model is not needed, just the previous layers to extract feature results.

By preference you may include more layers, but for quicker results avoid adding the unnecessary layers.

#Extract Image Features
"""

# extract features from image
features = {}
directory = os.path.join(BASE_DIR, 'images')

for img_name in tqdm(os.listdir(directory)):
    # load the image from file
    img_path = directory + '/' + img_name
    image = load_img(img_path, target_size=(224, 224))
    # convert image pixels to numpy array
    image = img_to_array(image)
    # reshape data for model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    # preprocess image for vgg
    image = preprocess_input(image)
    # extract features
    feature = model.predict(image, verbose=0)
    # get image ID
    image_id = img_name.split('.')[0]
    # store feature
    features[image_id] = feature

"""Dictionary 'features' is created and will be loaded with the extracted features of image data

load_img(img_path, target_size=(224, 224)) - custom dimension to resize the image when loaded to the array

image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) - reshaping the image data to preprocess in a RGB type image.

model.predict(image, verbose=0) - extraction of features from the image

img_name.split('.')[0] - split of the image name from the extension to load only the image name.
"""

# store features in pickle
pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))

"""Extracted features are not stored in the disk, so re-extraction of features can extend running time

Dumps and store your dictionary in a pickle for reloading it to save time
"""

# load features from pickle
with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:
    features = pickle.load(f)

"""Load all your stored feature data to your project for quicker runtime

# Load the Captions Data
"""

with open(os.path.join(BASE_DIR, 'caption.txt'), 'r') as f:
    next(f)
    captions_doc = f.read()

"""# Now we split and append the captions data with the image"""

# create mapping of image to captions
mapping = {}
# process lines
for line in tqdm(captions_doc.split('\n')):
    # split the line by comma(,)
    tokens = line.split(',')
    if len(line) < 2:
        continue
    image_id, caption = tokens[0], tokens[1:]
    # remove extension from image ID
    image_id = image_id.split('.')[0]
    # convert caption list to string
    caption = " ".join(caption)
    # create list if needed
    if image_id not in mapping:
        mapping[image_id] = []
    # store the caption
    mapping[image_id].append(caption)

"""Dictionary 'mapping' is created with key as image_id and values as the corresponding caption text

Same image may have multiple captions, if image_id not in mapping: mapping[image_id] = [] creates a list for appending captions to the corresponding image
"""

len(mapping)

"""# Preprocess Text Data"""

def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):
            # take one caption at a time
            caption = captions[i]
            # preprocessing steps
            # convert to lowercase
            caption = caption.lower()
            # delete digits, special chars, etc.,
            caption = caption.replace('[^A-Za-z]', '')
            # delete additional spaces
            caption = caption.replace('\s+', ' ')
            # add start and end tags to the caption
            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word)>1]) + ' endseq'
            captions[i] = caption

"""Defined to clean and convert the text for quicker process and better results"""

# before preprocess of text
mapping['5']

# preprocess the text
clean(mapping)

# after preprocess of text
mapping['5']

all_captions = []
for key in mapping:
    for caption in mapping[key]:
        all_captions.append(caption)

len(all_captions)

all_captions[:10]

"""# Processing of Text Data"""

# tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1

# Save the tokenizer

with open(os.path.join(WORKING_DIR, 'tokenizer.pkl'), 'wb') as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

# Load the tokenizer
with open(os.path.join(WORKING_DIR, 'tokenizer.pkl'), 'rb') as tokenizer_file:
    tokenizer = pickle.load(tokenizer_file)

vocab_size

# get maximum length of the caption available
max_length = max(len(caption.split()) for caption in all_captions)
max_length

"""Finding the maximum length of the captions, used for reference for the padding sequence.

# Train Test Split
"""

image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]

"""# Define a batch and include the padding sequence"""

# create data generator to get data in batch (avoids session crash)
def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    # loop over images
    X1, X2, y = list(), list(), list()
    n = 0
    while 1:
        for key in data_keys:
            n += 1
            captions = mapping[key]
            # process each caption
            for caption in captions:
                # encode the sequence
                seq = tokenizer.texts_to_sequences([caption])[0]
                # split the sequence into X, y pairs
                for i in range(1, len(seq)):
                    # split into input and output pairs
                    in_seq, out_seq = seq[:i], seq[i]
                    # pad input sequence
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    # encode output sequence
                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]
                    # store the sequences
                    X1.append(features[key][0])
                    X2.append(in_seq)
                    y.append(out_seq)
            if n == batch_size:
                X1, X2, y = np.array(X1), np.array(X2), np.array(y)
                yield [X1, X2], y
                X1, X2, y = list(), list(), list()
                n = 0

"""Padding sequence normalizes the size of all captions to the max size filling them with zeros for better results.

# **Model Creation**
"""

# encoder model
# image feature layers
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
# sequence feature layers
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(256)(se2)

# decoder model
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

# plot the model
plot_model(model, show_shapes=True)

"""shape=(4096,) - output length of the features from the VGG model

Dense - single dimension linear layer array

Dropout() - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers

model.compile() - compilation of the model

loss=’sparse_categorical_crossentropy’ - loss function for category outputs

optimizer=’adam’ - automatically adjust the learning rate for the model over the no. of epochs

Model plot shows the concatenation of the inputs and outputs into a single layer

Feature extraction of image was already done using VGG, no CNN model was needed in this step.

# Train Model
"""

# train the model
epochs = 30
batch_size = 32
steps = len(train) // batch_size

for i in range(epochs):
    # create data generator
    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
    # fit for one epoch
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)

"""steps = len(train) // batch_size - back propagation and fetch the next data

Loss decreases gradually over the iterations

Increase the no. of epochs for better results
"""

# save the model
model.save(WORKING_DIR+'/best_model.h5')

"""# Generate Captions for the Image"""

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

"""Convert the predicted index from the model into a word"""

# generate caption for an image
def predict_caption(model, image, tokenizer, max_length):
    # add start tag for generation process
    in_text = 'startseq'
    # iterate over the max length of sequence
    for i in range(max_length):
        # encode input sequence
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        # pad the sequence
        sequence = pad_sequences([sequence], max_length)
        # predict next word
        yhat = model.predict([image, sequence], verbose=0)
        # get index with high probability
        yhat = np.argmax(yhat)
        # convert index to word
        word = idx_to_word(yhat, tokenizer)
        # stop if word not found
        if word is None:
            break
        # append word as input for generating next word
        in_text += " " + word
        # stop if we reach end tag
        if word == 'endseq':
            break
    return in_text

"""Captiongenerator appending all the words for an image

The caption starts with 'startseq' and the model continues to predict the caption until the 'endseq' appeared

# Model Validation
"""

from nltk.translate.bleu_score import corpus_bleu
# validate with test data
actual, predicted = list(), list()

for key in tqdm(test):
    # get actual caption
    captions = mapping[key]
    # predict the caption for image
    y_pred = predict_caption(model, features[key], tokenizer, max_length)
    # split into words
    actual_captions = [caption.split() for caption in captions]
    y_pred = y_pred.split()
    # append to the list
    actual.append(actual_captions)
    predicted.append(y_pred)
# calcuate BLEU score
print("BLEU-1: %f" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))
print("BLEU-2: %f" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))

"""BLEU Score is used to evaluate the predicted text against a reference text, in a list of tokens.

The reference text contains all the words appended from the captions data (actual_captions)

A BLEU Score more than 0.4 is considered a good result, for a better score increase the no. of epochs accordingly.

# Visualize the Results
"""

from PIL import Image
import matplotlib.pyplot as plt
def generate_caption(image_name):
    # load the image
    image_id = image_name.split('.')[0]
    img_path = os.path.join(BASE_DIR, "images", image_name)
    image = Image.open(img_path)
    captions = mapping[image_id]

    # predict the caption
    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)
    print('--------------------Predicted--------------------')
    print(y_pred)
    plt.imshow(image)

generate_caption("57.jpg")



generate_caption("1005.jpeg")

generate_caption("12.jpg")

generate_caption("45.jpg")

generate_caption("14.jpg")

generate_caption("381.jpg")

generate_caption("64.jpg")

!pip install streamlit -q

!wget -q -O - ipv4.icanhazip.com

! streamlit run ICG.py & npx localtunnel --port 8501